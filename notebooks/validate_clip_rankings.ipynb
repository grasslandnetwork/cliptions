{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clip_model():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "    return model, preprocess, device\n",
    "\n",
    "def calculate_similarity(model, text1, text2, image_path, device):\n",
    "    # Load and preprocess image\n",
    "    image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Encode texts\n",
    "    text_features1 = model.encode_text(clip.tokenize(text1).to(device))\n",
    "    text_features2 = model.encode_text(clip.tokenize(text2).to(device))\n",
    "    image_features = model.encode_image(image)\n",
    "    \n",
    "    # Calculate similarities\n",
    "    sim1 = torch.cosine_similarity(text_features1, image_features)\n",
    "    sim2 = torch.cosine_similarity(text_features2, image_features)\n",
    "    \n",
    "    return sim1.item(), sim2.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases\n",
    "suspicious_guess = \"[-h]\"\n",
    "legitimate_guess = \"Cat sanctuary with woman wearing snoopy sweater\"\n",
    "round0_image_path = \"../rounds/round0/targetframe20250223_133057EST.jpg\"  # You'll need to add the actual path\n",
    "\n",
    "# Compare with other meaningless strings\n",
    "test_cases = [\n",
    "    \"[-h]\",\n",
    "    \"[--]\",\n",
    "    \"[-]\",\n",
    "    \"[h]\",\n",
    "    \"h\",\n",
    "    \"-\",\n",
    "    \"Cat sanctuary with woman wearing snoopy sweater\",\n",
    "    \"A cat shelter\",\n",
    "    \"Empty room\",\n",
    "    \"Random text\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Initialize model and get preprocessing function\n",
    "model, preprocess, device = load_clip_model()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity scores:\n",
      "'[-h]': 0.180\n",
      "'Cat sanctuary with woman wearing snoopy sweater': 0.278\n",
      "\n",
      "Ratio of suspicious to legitimate score: 0.649\n"
     ]
    }
   ],
   "source": [
    "# Test the specific anomaly case\n",
    "sim_suspicious, sim_legitimate = calculate_similarity(\n",
    "    model,\n",
    "    suspicious_guess,\n",
    "    legitimate_guess,\n",
    "    round0_image_path,\n",
    "    device\n",
    ")\n",
    "\n",
    "print(f\"Similarity scores:\")\n",
    "print(f\"'{suspicious_guess}': {sim_suspicious:.3f}\")\n",
    "print(f\"'{legitimate_guess}': {sim_legitimate:.3f}\")\n",
    "print(f\"\\nRatio of suspicious to legitimate score: {sim_suspicious/sim_legitimate:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All test cases ranked by similarity:\n",
      "--------------------------------------------------\n",
      "'Cat sanctuary with woman wearing snoopy sweater': 27.824\n",
      "'A cat shelter': 21.721\n",
      "'[h]': 18.952\n",
      "'[-]': 18.207\n",
      "'Empty room': 18.168\n",
      "'[--]': 18.156\n",
      "'-': 18.068\n",
      "'[-h]': 18.047\n",
      "'h': 17.729\n",
      "'Random text': 17.467\n"
     ]
    }
   ],
   "source": [
    "# Test all cases against the image\n",
    "results = []\n",
    "for text in test_cases:\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(clip.tokenize(text).to(device))\n",
    "        image_features = model.encode_image(preprocess(Image.open(round0_image_path)).unsqueeze(0).to(device))\n",
    "        \n",
    "        # Normalize features\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Calculate similarity\n",
    "        similarity = (100.0 * text_features @ image_features.T).item()\n",
    "        results.append((text, similarity))\n",
    "\n",
    "# Sort by similarity score\n",
    "results.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"All test cases ranked by similarity:\")\n",
    "print(\"-\" * 50)\n",
    "for text, sim in results:\n",
    "    print(f\"'{text}': {sim:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison for '[-h]':\n",
      "Original CLIP: 18.047\n",
      "OpenCLIP (LAION): 22.719\n"
     ]
    }
   ],
   "source": [
    "import open_clip\n",
    "\n",
    "def compare_clip_implementations(image_path, text):\n",
    "    # Original CLIP\n",
    "    clip_model, clip_preprocess, device = load_clip_model()\n",
    "    \n",
    "    # OpenCLIP (ViT-B/32)\n",
    "    openclip_model, _, openclip_preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')\n",
    "    openclip_tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
    "    \n",
    "    # Prepare inputs\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    # Original CLIP\n",
    "    clip_image = clip_preprocess(image).unsqueeze(0).to(device)\n",
    "    clip_text = clip.tokenize(text).to(device)\n",
    "    with torch.no_grad():\n",
    "        clip_image_features = clip_model.encode_image(clip_image)\n",
    "        clip_text_features = clip_model.encode_text(clip_text)\n",
    "        clip_sim = torch.cosine_similarity(clip_text_features, clip_image_features).item()\n",
    "    \n",
    "    # OpenCLIP\n",
    "    openclip_image = openclip_preprocess(image).unsqueeze(0).to(device)\n",
    "    openclip_text = openclip_tokenizer(text).to(device)\n",
    "    with torch.no_grad():\n",
    "        openclip_image_features = openclip_model.encode_image(openclip_image)\n",
    "        openclip_text_features = openclip_model.encode_text(openclip_text)\n",
    "        openclip_sim = torch.cosine_similarity(openclip_text_features, openclip_image_features).item()\n",
    "    \n",
    "    return {\n",
    "        'Original CLIP': clip_sim * 100,\n",
    "        'OpenCLIP (LAION)': openclip_sim * 100\n",
    "    }\n",
    "\n",
    "# Test our problem case\n",
    "results = compare_clip_implementations(\n",
    "    round0_image_path,\n",
    "    \"[-h]\"\n",
    ")\n",
    "\n",
    "print(f\"Comparison for '[-h]':\")\n",
    "for model_name, score in results.items():\n",
    "    print(f\"{model_name}: {score:.3f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text                                               |  Raw Score | Adjusted Score\n",
      "--------------------------------------------------------------------------------\n",
      "Cat sanctuary with woman wearing snoopy sweater    |     27.824 |         12.362\n",
      "A cat shelter                                      |     21.721 |          4.951\n",
      "[h]                                                |     18.952 |          1.588\n",
      "[-]                                                |     18.207 |          0.684\n",
      "Empty room                                         |     18.168 |          0.637\n",
      "[--]                                               |     18.156 |          0.622\n",
      "-                                                  |     18.068 |          0.516\n",
      "[-h]                                               |     18.047 |          0.490\n",
      "h                                                  |     17.729 |          0.104\n",
      "Random text                                        |     17.467 |         -0.214\n"
     ]
    }
   ],
   "source": [
    "# New function to calculate baseline\n",
    "def calculate_baseline(model, device, baseline_text=\"[UNUSED]\"):\n",
    "    with torch.no_grad():\n",
    "        baseline_features = model.encode_text(clip.tokenize(baseline_text).to(device))\n",
    "        baseline_features /= baseline_features.norm(dim=-1, keepdim=True)\n",
    "    return baseline_features\n",
    "\n",
    "# Modified test function with baseline adjustment\n",
    "def run_tests_with_baseline(image_path, test_cases):\n",
    "    model, preprocess, device = load_clip_model()\n",
    "    baseline = calculate_baseline(model, device)\n",
    "    \n",
    "    image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "    image_features = model.encode_image(image)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    results = []\n",
    "    for text in test_cases:\n",
    "        text_features = model.encode_text(clip.tokenize(text).to(device))\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        raw_score = (text_features @ image_features.T).item()\n",
    "        baseline_score = (baseline @ image_features.T).item()\n",
    "        adjusted_score = (raw_score - baseline_score) / (1 - baseline_score)\n",
    "        \n",
    "        results.append((text, raw_score*100, adjusted_score*100))\n",
    "    \n",
    "    return sorted(results, key=lambda x: x[2], reverse=True)\n",
    "\n",
    "# Run tests with baseline adjustment\n",
    "adjusted_results = run_tests_with_baseline(round0_image_path, test_cases)\n",
    "\n",
    "# Print formatted results\n",
    "print(f\"{'Text':<50} | {'Raw Score':>10} | {'Adjusted Score':>14}\")\n",
    "print(\"-\"*80)\n",
    "for text, raw, adj in adjusted_results:\n",
    "    print(f\"{text[:48]:<50} | {raw:10.3f} | {adj:14.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
